{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-WhKcDCizxD"
      },
      "source": [
        "**Objective**: Set up UMBRELA using Colab notebook (w/ GPT & HuggingFace Open-Source Models for eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5KE0l9Skoy_"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1.   Install Anserini\n",
        "2.   Install Pyserini\n",
        "3.   Install UMBRELA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WkCQV44lctB"
      },
      "source": [
        "**Anserini:** Need Java 21 and Maven 3.9+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKk72S_lllgK"
      },
      "source": [
        "Get JDK 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiEFBM_3pgcG"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-21-jdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5yZV_MpwScd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-21-openjdk-amd64'\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':' + os.environ['JAVA_HOME'] + '/bin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aptl-lbjlrhA"
      },
      "source": [
        "Confirm Java Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cSuDC2Eltxq"
      },
      "outputs": [],
      "source": [
        "!java -version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9I5nJ8el_7q"
      },
      "source": [
        "Get Maven (Latest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_H2eS_QlQGL"
      },
      "outputs": [],
      "source": [
        "!wget https://dlcdn.apache.org/maven/maven-3/3.9.11/binaries/apache-maven-3.9.11-bin.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIgH2fb4mLaK"
      },
      "outputs": [],
      "source": [
        "!tar -xvzf apache-maven-3.9.11-bin.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jic8wmbrTqSn"
      },
      "outputs": [],
      "source": [
        "!sudo mv apache-maven-3.9.11 /opt/maven"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SNqpJRNULHq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MAVEN_HOME'] = '/opt/maven'\n",
        "os.environ['PATH'] = os.environ['MAVEN_HOME'] + '/bin:' + os.environ['PATH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5bVsqDPl1DR"
      },
      "source": [
        "Confirm Maven is present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYAx6cxySu_w"
      },
      "outputs": [],
      "source": [
        "!mvn --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2FiNaIgmirf"
      },
      "source": [
        "Clone Anserini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM8kVgFNikFZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/anserini.git --recurse-submodules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrs7o1emm1bU"
      },
      "source": [
        "Follow along the Anserini official repo installation process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUM4wTzhn-gL"
      },
      "outputs": [],
      "source": [
        "%cd anserini/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdzlQwCam0uG"
      },
      "outputs": [],
      "source": [
        "!git submodule update --init --recursive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_LaQux1myBL"
      },
      "outputs": [],
      "source": [
        "!mvn clean package -DskipTests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0f3fO5GnGDi"
      },
      "outputs": [],
      "source": [
        "!cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6ED0OT-nKcQ"
      },
      "outputs": [],
      "source": [
        "!cd tools/eval/ndeval && make && cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMvwKt4ivsje"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnUX1lNNtlpg"
      },
      "source": [
        "**Pyserini**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3plhqdG3Icv"
      },
      "source": [
        "Follow development installation instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp2h9wTFv2CV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/pyserini.git --recurse-submodules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfvxATNR3CDr"
      },
      "outputs": [],
      "source": [
        "%cd pyserini/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxH2_xI3D2H"
      },
      "outputs": [],
      "source": [
        "!cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxpqrV9U3QBW"
      },
      "outputs": [],
      "source": [
        "!cd tools/eval/ndeval && make && cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43BUfZA43Tmt"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKjf8wa83w9s"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYcg3b3X7POp"
      },
      "outputs": [],
      "source": [
        "%cd anserini/target/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcQ5CWwc7noE"
      },
      "outputs": [],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up16x601nzYe"
      },
      "outputs": [],
      "source": [
        "!mv anserini-1.1.2-SNAPSHOT-fatjar.jar /content/pyserini/pyserini/resources/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S47mnN8ZbIbo"
      },
      "outputs": [],
      "source": [
        "%cd ../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlE8bwrTnmZL"
      },
      "outputs": [],
      "source": [
        "%cd pyserini/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDwPjiK08Riu"
      },
      "outputs": [],
      "source": [
        "#!python -m unittest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QulmRYktinCE"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ILNOApmoRU7"
      },
      "source": [
        "Set up UMBRELA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuaqpBUIo3nQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/umbrela.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_I6E0Yb_NQM"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRTqw_vE3VAv"
      },
      "outputs": [],
      "source": [
        "%cd /content/umbrela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3OBokun_gv5"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToJL8yN__j1h"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyaoZYCPGaa"
      },
      "source": [
        "Now, there are a dependency that need to be installed for the eval command to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORsbpqYAPJ5s"
      },
      "outputs": [],
      "source": [
        "!pip install retry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb6zqDNMU44V"
      },
      "source": [
        "# **FOR CHATGPT**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML28C8gtOo9P"
      },
      "outputs": [],
      "source": [
        "%cd src/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlIIOLB3_lok"
      },
      "outputs": [],
      "source": [
        "%%writefile chatgptrun.py\n",
        "from umbrela.gpt_judge import GPTJudge\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "judge_gpt = GPTJudge(qrel=\"dl19-passage\", prompt_type=\"bing\", model_name=\"gpt-4o\")\n",
        "\n",
        "input_dict = {\n",
        "    \"query\": {\"text\": \"how long is life cycle of flea\", \"qid\": \"264014\"},\n",
        "    \"candidates\": [\n",
        "        {\n",
        "            \"doc\": {\n",
        "                \"segment\": \"The life cycle of a flea can last anywhere from 20 days to an entire year. It depends on how long the flea remains in the dormant stage (eggs, larvae, pupa). Outside influences, such as weather, affect the flea cycle. A female flea can lay around 20 to 25 eggs in one day.\"\n",
        "            },\n",
        "            \"docid\": \"4834547\",\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "judgments = judge_gpt.judge(request_dict=input_dict)\n",
        "print(judgments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocmKCenoEbab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPEN_AI_API_KEY'] = 'sk-dummy-key-for-testing'\n",
        "os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://dummy.openai.azure.com/'\n",
        "os.environ['AZURE_OPENAI_API_VERSION'] = '2024-02-15-preview'\n",
        "os.environ['AZURE_OPENAI_API_KEY'] = 'dummy-key-for-testing'\n",
        "os.environ['AZURE_OPENAI_API_BASE'] = 'https://dummy.openai.azure.com/'\n",
        "os.environ['DEPLOYMENT_NAME'] = 'gpt-4o'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwRtowbqBEjF"
      },
      "outputs": [],
      "source": [
        "!python chatgptrun.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error, as expected, since I did not use valid values for the env variables. However, generally, it would work otherwise."
      ],
      "metadata": {
        "id": "3-oH8j-hJuID"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47QopzVVBIf"
      },
      "source": [
        "# **FOR HUGGING FACE OPEN-SOURCE MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-UJSiMNP8aV"
      },
      "source": [
        "Below is a Claude-generated prompt to test small, medium, large open-source models on Hugging Face with judgement snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk6I505CyWGf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"....................\"\n",
        "os.environ[\"HF_CACHE_DIR\"] = \"/content/hf_cache\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile huggingfacerun.py\n",
        "\"\"\"\n",
        "UMBRELA 30-Minute Benchmark for Colab Pro Decision\n",
        "Optimized to test real TREC evaluation performance in minimal time\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import psutil\n",
        "import subprocess\n",
        "import os\n",
        "import re # Import regex for parsing model output\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration, T5Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class QuickUmbrelaBenchmark:\n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "\n",
        "        # Real TREC DL 2019 samples (from UMBRELA paper)\n",
        "        self.trec_samples = [\n",
        "            {\n",
        "                \"query\": \"how long is life cycle of flea\",\n",
        "                \"passage\": \"The life cycle of a flea can last anywhere from 20 days to an entire year. It depends on how long the flea remains in the dormant stage (eggs, larvae, pupa). Outside influences, such as weather, affect the flea cycle. A female flea can lay around 20 to 25 eggs in one day.\",\n",
        "                \"expected_relevance\": 3  # From UMBRELA paper\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"medicare's definition of mechanical ventilation\",\n",
        "                \"passage\": \"Continuous Positive Airway Pressure (CPAP) Continuous positive airway pressure ‚Äì also called CPAP ‚Äì is a treatment in which a mask is worn over the nose and/or mouth while you sleep. The mask is hooked up to a machine that delivers a continuous flow of air into the nose.\",\n",
        "                \"expected_relevance\": 1\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"what is the daily life of thai people\",\n",
        "                \"passage\": \"Thai Flag Meaning: The red stripes represent the blood spilt to maintain Thailand's independence. The white stands for purity and is the color of Buddhism which is the country's main religion. The flag of Thailand consists of five horizontal stripes.\",\n",
        "                \"expected_relevance\": 0\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"define visceral pleura\",\n",
        "                \"passage\": \"The visceral pleura is the thin membrane that covers the surface of the lungs. It is continuous with the parietal pleura, which lines the chest cavity. The pleural space between these two membranes contains pleural fluid.\",\n",
        "                \"expected_relevance\": 3\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"causes of air pollution\",\n",
        "                \"passage\": \"Air pollution is caused by various factors including vehicle emissions, industrial activities, burning of fossil fuels, deforestation, and agricultural practices. These activities release harmful substances into the atmosphere.\",\n",
        "                \"expected_relevance\": 2\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Models to test with approximate parameter counts (M = million, B = billion)\n",
        "        self.test_models = [\n",
        "            {\n",
        "                \"name\": \"google/flan-t5-small\",\n",
        "                \"type\": \"t5\",\n",
        "                \"size\": \"Small\",\n",
        "                \"params_m\": 80,\n",
        "                \"description\": \"Smallest T5, instruction-tuned\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/flan-t5-base\",\n",
        "                \"type\": \"t5\",\n",
        "                \"size\": \"Base\",\n",
        "                \"params_m\": 250,\n",
        "                \"description\": \"Mid-range T5\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/flan-t5-large\",\n",
        "                \"type\": \"t5\",\n",
        "                \"size\": \"Large\",\n",
        "                \"params_m\": 780,\n",
        "                \"description\": \"Larger T5\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/flan-t5-xl\",\n",
        "                \"type\": \"t5\",\n",
        "                \"size\": \"Extra Large\",\n",
        "                \"params_b\": 3, # 3 Billion\n",
        "                \"description\": \"Significant T5 model for better judgments\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"microsoft/DialoGPT-medium\",\n",
        "                \"type\": \"gpt\",\n",
        "                \"size\": \"Medium\",\n",
        "                \"params_m\": 345, # From previous info\n",
        "                \"description\": \"Alternative GPT model\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "                \"type\": \"gpt\", # Phi-3 models are decoder-only, like GPT\n",
        "                \"size\": \"Mini (Instruct)\",\n",
        "                \"params_b\": 3.8, # 3.8 Billion parameters\n",
        "                \"description\": \"Microsoft's small, capable instruct model\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/gemma-2-9b\",\n",
        "                \"type\": \"gpt\", # Gemma models are decoder-only\n",
        "                \"size\": \"9B\",\n",
        "                \"params_b\": 9,\n",
        "                \"description\": \"Gemma 9 Billion instruction-tuned\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Qwen/Qwen2-7B-Instruct\",\n",
        "                \"type\": \"gpt\", # Qwen models are decoder-only (causal LM)\n",
        "                \"size\": \"7B (Instruct)\",\n",
        "                \"params_b\": 7,\n",
        "                \"description\": \"Qwen 7 Billion instruction-tuned\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"HuggingFaceM4/idefics-9b-instruct\",\n",
        "                \"type\": \"gpt\", # IDEFICS is decoder-only, with multimodal capabilities\n",
        "                \"size\": \"9B (Instruct, Multi)\",\n",
        "                \"params_b\": 9,\n",
        "                \"description\": \"IDEFICS 9 Billion instruction-tuned, multimodal\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"utter-project/EuroLLM-9B-Instruct\",\n",
        "                \"type\": \"gpt\", # EuroLLM is decoder-only\n",
        "                \"size\": \"9B (Instruct, Multi-lingual)\",\n",
        "                \"params_b\": 9,\n",
        "                \"description\": \"EuroLLM 9 Billion instruction-tuned\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"01-ai/Yi-1.5-9B-Chat\",\n",
        "                \"type\": \"gpt\", # Yi models are decoder-only (chat variant)\n",
        "                \"size\": \"9B (Chat)\",\n",
        "                \"params_b\": 9,\n",
        "                \"description\": \"Yi 9 Billion chat model\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "                \"type\": \"gpt\", # Llama models are decoder-only\n",
        "                \"size\": \"8B (Instruct)\",\n",
        "                \"params_b\": 8,\n",
        "                \"description\": \"Meta Llama 3 8 Billion instruction-tuned\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def check_system_resources(self):\n",
        "        \"\"\"Check available system resources and determine Colab tier\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üîç SYSTEM RESOURCES & COLAB TIER DETECTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # RAM\n",
        "        ram = psutil.virtual_memory()\n",
        "        print(f\"üíæ RAM: {ram.available/1024**3:.1f}GB available / {ram.total/1024**3:.1f}GB total\")\n",
        "\n",
        "        # GPU Detection\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name()\n",
        "            gpu_memory = torch.cuda.get_device_properties(0).total_memory/1024**3\n",
        "            gpu_available = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1024**3\n",
        "\n",
        "            print(f\"üöÄ GPU: {gpu_name}\")\n",
        "            print(f\"üìä GPU Memory: {gpu_memory:.1f}GB total, {gpu_available:.1f}GB available\")\n",
        "\n",
        "            # Determine Colab tier (updated for typical Colab GPU memory ranges)\n",
        "            if \"T4\" in gpu_name and gpu_memory < 16:\n",
        "                tier = \"üÜì Colab FREE (T4, ~15GB)\"\n",
        "            elif \"V100\" in gpu_name or (gpu_memory > 20 and gpu_memory < 35):\n",
        "                tier = \"üí∞ Colab PRO (V100, ~32GB)\"\n",
        "            elif \"A100\" in gpu_name or gpu_memory >= 35:\n",
        "                tier = \"üíé Colab PRO+ (A100, ~40GB)\"\n",
        "            else:\n",
        "                tier = f\"‚ùì Unknown ({gpu_name}, {gpu_memory:.1f}GB)\"\n",
        "\n",
        "            print(f\"üè∑Ô∏è  Detected: {tier}\")\n",
        "        else:\n",
        "            gpu_name = \"CPU Only\"\n",
        "            gpu_memory = 0\n",
        "            print(\"‚ùå GPU: Not available (CPU mode)\")\n",
        "            tier = \"CPU Only\"\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        return {\"name\": gpu_name, \"memory\": gpu_memory, \"tier\": tier}\n",
        "\n",
        "    def create_umbrela_prompt(self, query, passage):\n",
        "        \"\"\"\n",
        "        Create UMBRELA-style prompt based on Figure 1 of the paper,\n",
        "        tailored to encourage numerical output in the specified format.\n",
        "        \"\"\"\n",
        "        prompt_template = \"\"\"Given a query and a passage, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
        "0 = Irrelevant: The passage has nothing to do with the query.\n",
        "1 = Related: The passage seems related to the query but does not answer it.\n",
        "2 = Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.\n",
        "3 = Perfectly relevant: The passage is dedicated to the query and contains the exact answer.\n",
        "\n",
        "Important Instruction: Assign category 1 if the passage is somewhat related to the topic but not completely, category 2 if passage presents something very important related to the entire topic and also has some extra information and category 3 if the passage only and entirely refers to the topic. If none of the above satisfies give it category 0.\n",
        "\n",
        "Query: {query}\n",
        "Passage: {passage}\n",
        "\n",
        "Do not provide any code in result. Provide each score in the format of: ##final score: score without providing any reasoning.\n",
        "##final score: \"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(query=query, passage=passage)\n",
        "        return prompt\n",
        "\n",
        "    def test_model_performance(self, model_info, num_samples=80): # Default samples set to 80\n",
        "        \"\"\"Test a specific model with timing and memory benchmarks and check judgment accuracy\"\"\"\n",
        "        model_name = model_info[\"name\"]\n",
        "        model_type = model_info[\"type\"]\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üß™ TESTING: {model_name}\")\n",
        "        print(f\"üìã Type: {model_type.upper()} | Size: {model_info['size']}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        try:\n",
        "            # Memory before loading\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                mem_before = torch.cuda.memory_allocated() / 1024**3\n",
        "            else:\n",
        "                mem_before = 0\n",
        "\n",
        "            # Load model\n",
        "            print(\"‚è≥ Loading model...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Use AutoModelForSeq2SeqLM for T5 models (encoder-decoder)\n",
        "            # Use AutoModelForCausalLM for GPT-style models (decoder-only)\n",
        "            if model_type == \"t5\":\n",
        "                tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "                model = T5ForConditionalGeneration.from_pretrained(\n",
        "                    model_name,\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "                )\n",
        "            else:  # GPT-style\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                # Specific models that might require trust_remote_code=True\n",
        "                if any(m in model_name for m in [\"Phi-3\", \"Qwen\", \"idefics\", \"EuroLLM\", \"Yi\"]):\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_name,\n",
        "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                        trust_remote_code=True\n",
        "                    )\n",
        "                else:\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_name,\n",
        "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                        device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "                    )\n",
        "                if tokenizer.pad_token is None:\n",
        "                    tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set for generation\n",
        "\n",
        "            load_time = time.time() - start_time\n",
        "\n",
        "            # Memory after loading\n",
        "            if torch.cuda.is_available():\n",
        "                mem_after = torch.cuda.memory_allocated() / 1024**3\n",
        "                memory_used = mem_after - mem_before\n",
        "            else:\n",
        "                memory_used = 0\n",
        "\n",
        "            print(f\"‚úÖ Loaded in {load_time:.1f}s | Memory: {memory_used:.2f}GB\")\n",
        "\n",
        "            # Benchmark evaluations\n",
        "            print(f\"üîÑ Running {num_samples} evaluations...\")\n",
        "\n",
        "            eval_times = []\n",
        "            correct_judgments_count = 0\n",
        "            sample_outputs = []\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                # Cycle through our TREC samples\n",
        "                sample = self.trec_samples[i % len(self.trec_samples)]\n",
        "                query = sample[\"query\"]\n",
        "                passage = sample[\"passage\"]\n",
        "\n",
        "                eval_start = time.time()\n",
        "\n",
        "                try:\n",
        "                    prompt = self.create_umbrela_prompt(query, passage) # Prompt is now generic\n",
        "\n",
        "                    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
        "                    if torch.cuda.is_available():\n",
        "                        inputs = inputs.to(model.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model.generate(\n",
        "                            inputs,\n",
        "                            max_new_tokens=20, # Increased from 3/5 to allow more generation\n",
        "                            temperature=0.1,  # Keep responses deterministic\n",
        "                            do_sample=False,\n",
        "                            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "                        )\n",
        "\n",
        "                    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                    # Attempt to parse the expected format \"##final score: X\"\n",
        "                    parsed_response = None\n",
        "                    match = re.search(r'##final score:\\s*(\\d+)', full_response)\n",
        "                    if match:\n",
        "                        try:\n",
        "                            parsed_response = int(match.group(1).strip())\n",
        "                        except ValueError:\n",
        "                            pass # Not a valid integer\n",
        "\n",
        "                    # Prepare response for display\n",
        "                    display_response = str(parsed_response) if parsed_response is not None else full_response[len(tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()\n",
        "                    display_response = display_response[:20] if len(display_response) > 20 else display_response # Truncate if too long\n",
        "\n",
        "                    is_correct = (parsed_response == sample[\"expected_relevance\"])\n",
        "                    if is_correct:\n",
        "                        correct_judgments_count += 1\n",
        "\n",
        "                    eval_time = time.time() - eval_start\n",
        "                    eval_times.append(eval_time)\n",
        "\n",
        "                    if i < 3: # Store first few examples for detailed output\n",
        "                        sample_outputs.append({\n",
        "                            \"query\": query[:50] + \"...\",\n",
        "                            \"response\": display_response,\n",
        "                            \"time\": eval_time,\n",
        "                            \"expected\": sample[\"expected_relevance\"],\n",
        "                            \"correct\": is_correct\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è Evaluation {i+1} failed: {str(e)[:50]}...\") # Truncate error message\n",
        "                    eval_times.append(float('inf')) # Record as infinite time for failed eval\n",
        "\n",
        "            # Filter out infinite times for avg calculation, but keep for success rate context\n",
        "            valid_eval_times = [t for t in eval_times if t != float('inf')]\n",
        "            avg_eval_time = sum(valid_eval_times) / max(len(valid_eval_times), 1)\n",
        "\n",
        "            # The 'success rate' now reflects the rate of judgments being parsed correctly and compared\n",
        "            # to expected. Not just execution success.\n",
        "            # A correct judgment rate is more meaningful than just successful execution.\n",
        "            correct_judgment_rate = (correct_judgments_count / num_samples) * 100 if num_samples > 0 else 0\n",
        "\n",
        "            # Project to full TREC dataset (9260 evaluations)\n",
        "            full_trec_evals = 9260\n",
        "            projected_hours = (avg_eval_time * full_trec_evals) / 3600\n",
        "            projected_minutes = ((avg_eval_time * full_trec_evals) % 3600) / 60\n",
        "\n",
        "            # Determine Colab recommendation\n",
        "            # NOTE: These thresholds are estimates. Actual performance varies.\n",
        "            if memory_used > 12: # Over 12GB VRAM often requires A100 (Pro+)\n",
        "                colab_rec = \"PRO+ Required\"\n",
        "                colab_reason = f\"Memory ({memory_used:.1f}GB) too high for Free/Pro\"\n",
        "            elif memory_used > 8: # Over 8GB VRAM often benefits from V100 (Pro)\n",
        "                colab_rec = \"PRO Recommended\"\n",
        "                colab_reason = f\"Memory ({memory_used:.1f}GB) ideal for Pro\"\n",
        "            elif projected_hours > 24:\n",
        "                colab_rec = \"PRO Recommended\"\n",
        "                colab_reason = f\"Time ({projected_hours:.1f}h) > 24 hours (Colab max lifetime)\"\n",
        "            elif projected_hours > 12:\n",
        "                colab_rec = \"PRO Helpful\"\n",
        "                colab_reason = f\"Time ({projected_hours:.1f}h) > 12 hours (Colab max lifetime)\"\n",
        "            else:\n",
        "                colab_rec = \"FREE OK\"\n",
        "                colab_reason = \"Within limits\"\n",
        "\n",
        "            # Print results\n",
        "            print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
        "            print(f\"  ‚ö° Avg evaluation: {avg_eval_time:.3f}s\")\n",
        "            print(f\"  ‚úÖ Correct Judgment Rate: {correct_judgment_rate}/{num_samples} ({correct_judgment_rate:.1f}%)\")\n",
        "            print(f\"  üíæ Memory used: {memory_used:.2f}GB\")\n",
        "            print(f\"  üïê Full TREC est: {projected_hours:.1f}h {projected_minutes:.0f}m\")\n",
        "            print(f\"  üéØ Colab rec: {colab_rec} ({colab_reason})\")\n",
        "            print(f\"Total model test completion: {(time.time() - start_time):.1f}s\") # Time for this model's test\n",
        "\n",
        "            # Show sample outputs\n",
        "            if sample_outputs:\n",
        "                print(f\"\\nüîç SAMPLE OUTPUTS:\")\n",
        "                for i, sample_out in enumerate(sample_outputs):\n",
        "                    correct_str = \"‚úîÔ∏è Correct\" if sample_out['correct'] else \"‚ùå Incorrect\"\n",
        "                    print(f\"  {i+1}. Query: {sample_out['query']}\")\n",
        "                    print(f\"       Response: '{sample_out['response']}' (expected: {sample_out['expected']}) - {sample_out['time']:.3f}s [{correct_str}]\")\n",
        "\n",
        "            result = {\n",
        "                \"model_name\": model_name,\n",
        "                \"model_type\": model_type,\n",
        "                \"model_size\": model_info[\"size\"],\n",
        "                \"params_m\": model_info.get(\"params_m\"), # Use .get to handle missing key for XL (params_b)\n",
        "                \"params_b\": model_info.get(\"params_b\"),\n",
        "                \"load_time\": load_time,\n",
        "                \"memory_used_gb\": memory_used,\n",
        "                \"avg_eval_time\": avg_eval_time,\n",
        "                \"correct_judgment_rate\": correct_judgment_rate, # Updated field\n",
        "                \"projected_hours\": projected_hours,\n",
        "                \"colab_recommendation\": colab_rec,\n",
        "                \"colab_reason\": colab_reason\n",
        "            }\n",
        "\n",
        "            self.results.append(result)\n",
        "\n",
        "            # Cleanup\n",
        "            del model\n",
        "            del tokenizer\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error handling for model loading failures\n",
        "            print(f\"‚ùå FAILED to test {model_name}: {str(e)[:100]}...\") # Truncate long error messages\n",
        "            print(\"  This often means the model is too large for the available memory or the model name/type is incorrect.\")\n",
        "\n",
        "            # Add a partial result for failed models to the report\n",
        "            self.results.append({\n",
        "                \"model_name\": model_name,\n",
        "                \"model_type\": model_type,\n",
        "                \"model_size\": model_info[\"size\"],\n",
        "                \"params_m\": model_info.get(\"params_m\"),\n",
        "                \"params_b\": model_info.get(\"params_b\"),\n",
        "                \"load_time\": float('inf'), # Indicate failure for load time\n",
        "                \"memory_used_gb\": float('inf'), # Indicate unknown/exceeded memory\n",
        "                \"avg_eval_time\": float('inf'), # Indicate failure for eval time\n",
        "                \"correct_judgment_rate\": 0.0, # No correct judgments if failed\n",
        "                \"projected_hours\": float('inf'),\n",
        "                \"colab_recommendation\": \"FAIL: Check Memory/Model Name\",\n",
        "                \"colab_reason\": str(e)[:50]\n",
        "            })\n",
        "            return None\n",
        "\n",
        "    def run_quick_benchmark(self):\n",
        "        \"\"\"Run the optimized 30-minute benchmark\"\"\"\n",
        "        print(\"üöÄ QUICK UMBRELA BENCHMARK FOR COLAB PRO DECISION\")\n",
        "        print(\"üéØ Target: Evaluate TREC relevance assessment for multiple models\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # System check\n",
        "        system_info = self.check_system_resources()\n",
        "\n",
        "        # Determine sample size to keep total time around 15-16 minutes\n",
        "        # Sum of avg_eval_times for current models (estimated with XL) is ~11.071s\n",
        "        # (11.071s * num_samples) / 60s/min = ~15 minutes => num_samples = 900 / 11.071 = ~81\n",
        "        # With 11 models, 80 samples each: (roughly 45s per model test run) * 11 models / 60s/min = ~8.25 min\n",
        "        sample_size = 80 # Aim for 80 samples per model to keep total time within reasonable limits\n",
        "\n",
        "        print(f\"\\nüß™ Testing {len(self.test_models)} models with {sample_size} evaluations each\")\n",
        "        # Estimate based on a typical average run time for a model test, adjusted for number of models\n",
        "        estimated_total_time_minutes = (len(self.test_models) * 45) / 60 # Rough average of 45 seconds per model test\n",
        "        print(f\"‚è±Ô∏è  Estimated total benchmark time: ~{estimated_total_time_minutes:.1f} minutes\")\n",
        "\n",
        "        start_benchmark = time.time()\n",
        "\n",
        "        # Test each model\n",
        "        for i, model_info in enumerate(self.test_models, 1):\n",
        "            print(f\"\\n[{i}/{len(self.test_models)}] Testing {model_info['name']}\")\n",
        "\n",
        "            model_start = time.time()\n",
        "            result = self.test_model_performance(model_info, sample_size)\n",
        "            model_time = time.time() - model_start\n",
        "\n",
        "            if result:\n",
        "                print(f\"‚úÖ Model test completed in {model_time:.1f}s\")\n",
        "            else:\n",
        "                print(f\"‚ùå Model test failed after {model_time:.1f}s\")\n",
        "\n",
        "            # Brief cooldown to avoid immediate resource issues\n",
        "            time.sleep(2) # Increased cooldown slightly\n",
        "\n",
        "        total_time = time.time() - start_benchmark\n",
        "\n",
        "        # Generate report\n",
        "        self.generate_quick_report(system_info, total_time)\n",
        "\n",
        "    def generate_quick_report(self, system_info, total_time):\n",
        "        \"\"\"Generate focused report for Colab Pro decision, including parameter counts\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä COLAB PRO DECISION REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if not self.results:\n",
        "            print(\"‚ùå No results to analyze\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚è±Ô∏è  Benchmark completed in {total_time/60:.1f} minutes\")\n",
        "        print(f\"üñ•Ô∏è  System: {system_info['tier']}\")\n",
        "        print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        # Sort by performance (fastest avg_eval_time first, handle inf values)\n",
        "        sorted_results = sorted(self.results, key=lambda x: x[\"avg_eval_time\"] if x[\"avg_eval_time\"] != float('inf') else float('inf'))\n",
        "\n",
        "        print(f\"\\n{'MODEL PERFORMANCE RANKING':^80}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Rank':<5} {'Model':<25} {'Parameters':<12} {'Speed':<8} {'Mem Used':<10} {'Full TREC Est':<15} {'Judgements %':<15} {'Colab Rec':<15}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for i, result in enumerate(sorted_results, 1):\n",
        "            model_short = result[\"model_name\"].split(\"/\")[-1][:20]\n",
        "            params_str = \"\"\n",
        "            if result.get(\"params_m\") is not None:\n",
        "                params_str = f\"{result['params_m']}M\"\n",
        "            elif result.get(\"params_b\") is not None:\n",
        "                params_str = f\"{result['params_b']}B\"\n",
        "\n",
        "            mem_used_str = f\"{result['memory_used_gb']:.1f}GB\" if result['memory_used_gb'] != float('inf') else \"N/A\"\n",
        "            avg_eval_time_str = f\"{result['avg_eval_time']:.3f}s\" if result['avg_eval_time'] != float('inf') else \"N/A\"\n",
        "            projected_time_str = f\"{result['projected_hours']:.1f}h\" if result['projected_hours'] != float('inf') else \"N/A\"\n",
        "\n",
        "            # Corrected f-string for the 'Judgements %' column\n",
        "            correct_judgment_rate_val = result['correct_judgment_rate']\n",
        "            correct_judgment_rate_display = f\"{correct_judgment_rate_val:.1f}\"\n",
        "            if correct_judgment_rate_val == 0.0:\n",
        "                suffix_for_display = \"\"\n",
        "            elif correct_judgment_rate_val < 100.0:\n",
        "                suffix_for_display = \"%\"\n",
        "            else: # For 100.0%\n",
        "                suffix_for_display = \" \" # Add a space for 100% to maintain alignment as per original intent\n",
        "\n",
        "            # Ensure the padding aligns with the full string for Judgments %\n",
        "            # The padding is applied to the combined string.\n",
        "            formatted_judgement_percent_col = f\"{correct_judgment_rate_display}{suffix_for_display:<14}\"\n",
        "\n",
        "            print(f\"{i:<5} {model_short:<25} {params_str:<12} {avg_eval_time_str:<8} {mem_used_str:<10} {projected_time_str:<15} {formatted_judgement_percent_col} {result['colab_recommendation']:<15}\")\n",
        "\n",
        "        # Decision matrix\n",
        "        print(f\"\\n{'DECISION MATRIX':^80}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        free_ok_models = [r for r in sorted_results if \"FREE\" in r[\"colab_recommendation\"] and r[\"projected_hours\"] != float('inf')]\n",
        "        pro_helpful = [r for r in sorted_results if \"Helpful\" in r[\"colab_recommendation\"] and r[\"projected_hours\"] != float('inf')]\n",
        "        pro_recommended = [r for r in sorted_results if \"Recommended\" in r[\"colab_recommendation\"] and r[\"projected_hours\"] != float('inf')]\n",
        "        pro_required = [r for r in sorted_results if \"Required\" in r[\"colab_recommendation\"] and r[\"projected_hours\"] != float('inf')]\n",
        "        failed_models = [r for r in self.results if r[\"load_time\"] == float('inf')] # Use self.results to get all including failed ones\n",
        "\n",
        "        if free_ok_models:\n",
        "            best_free = free_ok_models[0]\n",
        "            print(f\"‚úÖ COLAB FREE is sufficient:\")\n",
        "            print(f\"   ‚Ä¢ Best model: {best_free['model_name'].split('/')[-1]} ({best_free.get('params_m', '')}M{best_free.get('params_b', '')}B parameters)\")\n",
        "            print(f\"   ‚Ä¢ Estimated time: {best_free['projected_hours']:.1f} hours\")\n",
        "            print(f\"   ‚Ä¢ Memory usage: {best_free['memory_used_gb']:.1f}GB\")\n",
        "            print(f\"   ‚Ä¢ Correct Judgment Rate: {best_free['correct_judgment_rate']:.1f}%\")\n",
        "            print(f\"   ‚Ä¢ üí∞ Cost: $0/month\")\n",
        "\n",
        "        if pro_recommended or pro_helpful:\n",
        "            relevant_models = pro_recommended + pro_helpful\n",
        "            # Filter out models that failed or are too slow for realistic Pro benefit\n",
        "            relevant_models = [m for m in relevant_models if m[\"projected_hours\"] < 24] # Only show if it's under 24 hours\n",
        "            if relevant_models:\n",
        "                best_pro = min(relevant_models, key=lambda x: x[\"projected_hours\"])\n",
        "                print(f\"\\n‚ö° COLAB PRO would provide:\")\n",
        "                print(f\"   ‚Ä¢ Better model options: {len(relevant_models)} models\")\n",
        "                print(f\"   ‚Ä¢ Potentially faster completion: {best_pro['projected_hours']:.1f} hours with {best_pro['model_name'].split('/')[-1]} ({best_pro.get('params_m', '')}M{best_pro.get('params_b', '')}B parameters)\")\n",
        "                print(f\"   ‚Ä¢ More reliable performance for larger models\")\n",
        "                print(f\"   ‚Ä¢ üí∏ Cost: $10/month\")\n",
        "\n",
        "        if pro_required:\n",
        "            print(f\"\\nüî¥ COLAB PRO+ needed for (memory-intensive models):\")\n",
        "            for model in pro_required:\n",
        "                print(f\"   ‚Ä¢ {model['model_name'].split('/')[-1]} ({model.get('params_m', '')}M{model.get('params_b', '')}B parameters): {model['colab_reason']}\")\n",
        "\n",
        "        if failed_models:\n",
        "            print(f\"\\n‚ùå FAILED TO RUN (Likely memory issues or invalid model name):\")\n",
        "            for model in failed_models:\n",
        "                print(f\"   ‚Ä¢ {model['model_name'].split('/')[-1]} ({model.get('params_m', '')}M{model.get('params_b', '')}B parameters): {model['colab_reason']}\")\n",
        "\n",
        "        # Final recommendation\n",
        "        print(f\"\\n{'üéØ FINAL RECOMMENDATION':^80}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if free_ok_models:\n",
        "            fastest_free = free_ok_models[0]\n",
        "            if fastest_free[\"projected_hours\"] < 12:\n",
        "                print(\"üí° VERDICT: Colab FREE is perfectly adequate for your current needs.\")\n",
        "                print(f\"   ‚úÖ You can complete a full TREC evaluation in {fastest_free['projected_hours']:.1f} hours using {fastest_free['model_name'].split('/')[-1]}.\")\n",
        "                print(f\"   ‚úÖ It achieves a correct judgment rate of {fastest_free['correct_judgment_rate']:.1f}%.\")\n",
        "                print(f\"   üí∞ Save $10/month - use the free tier!\")\n",
        "            else:\n",
        "                print(\"üí° VERDICT: Colab PRO recommended but not strictly required.\")\n",
        "                print(f\"   ‚ö†Ô∏è  Free tier will take {fastest_free['projected_hours']:.1f} hours with {fastest_free['model_name'].split('/')[-1]}, exceeding typical session limits.\")\n",
        "                print(f\"   ‚ö° Pro tier could significantly reduce this time and provide more stable performance.\")\n",
        "                print(f\"   üí∏ Consider Pro if your time and consistent access are more valuable than $10/month.\")\n",
        "        else:\n",
        "            print(\"üí° VERDICT: Colab PRO is necessary for any of the tested models.\")\n",
        "            print(\"   ‚ùå Free tier cannot handle the memory requirements of these models for benchmarking.\")\n",
        "            print(\"   ‚úÖ A Pro tier ($10/month) is required for reasonable performance and to even run some models.\")\n",
        "            print(\"   üí∏ This investment is justified for these tasks.\")\n",
        "\n",
        "        # Command for your specific use case\n",
        "        if free_ok_models:\n",
        "            best_model_for_command = free_ok_models[0][\"model_name\"]\n",
        "            print(f\"\\nüìù FOR YOUR UMBRELA COMMAND (using the best FREE tier compatible model):\")\n",
        "            print(\"-\" * 70)\n",
        "            print(f\"# Your command for efficient testing on Colab Free:\")\n",
        "            print(f\"!python running.py \\\\\") # Changed to running.py as this is the current script name\n",
        "            print(f\"  --qrel dl19-passage \\\\\") # These args would need to be parsed by running.py if you want to use them\n",
        "            print(f\"  --result_file output.txt \\\\\")\n",
        "            print(f\"  --prompt_type bing \\\\\")\n",
        "            print(f\"  --model {best_model_for_command} \\\\\")\n",
        "            print(f\"  --few_shot_count 0 \\\\\")\n",
        "            print(f\"  --device cuda \\\\\")\n",
        "            print(f\"  --num_sample 1\")\n",
        "            print(f\"# Estimated completion for full TREC evaluation with this model: {free_ok_models[0]['projected_hours']:.1f} hours\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Run the quick benchmark\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark = QuickUmbrelaBenchmark()\n",
        "    benchmark.run_quick_benchmark()\n"
      ],
      "metadata": {
        "id": "YP1L8xg2fl1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StTPDL4GWWOK"
      },
      "outputs": [],
      "source": [
        "!python huggingfacerun.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmisZ5d3tE9Y"
      },
      "source": [
        "Now, eval for complete judgement using hugging face open-source models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jzKiKGTtUjq"
      },
      "outputs": [],
      "source": [
        "%%writefile output.txt\n",
        "#test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "t8xP0WGC6Nr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by-xXtbDvAOt"
      },
      "outputs": [],
      "source": [
        "!python umbrela/hgfllm_judge.py --qrel dl19-passage --result_file output.txt --prompt_type bing --model Qwen/Qwen2-7B-Instruct --few_shot_count 0 --device cuda --num_sample 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}